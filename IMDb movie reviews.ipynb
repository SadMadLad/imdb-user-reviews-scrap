{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "divine-canada",
   "metadata": {},
   "source": [
    "# Imported Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "capital-proposition",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import requests\n",
    "from requests.exceptions import ConnectionError\n",
    "from requests.exceptions import Timeout\n",
    "from requests.exceptions import TooManyRedirects\n",
    "from requests.exceptions import RequestException\n",
    "from requests.exceptions import HTTPError as HTTPErrorRequests\n",
    "\n",
    "from json import loads\n",
    "from json import dump\n",
    "\n",
    "from html import unescape\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "apparent-humor",
   "metadata": {},
   "source": [
    "# Class Declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "provincial-hayes",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieIMDb:\n",
    "    def __init__(self, movieURL, parser='html.parser'):\n",
    "        self.movieURL = movieURL\n",
    "        self.movieID = self._extractMovieID()\n",
    "        \n",
    "        self.movieHomepageHTML = self._simpleConnector(self.movieURL)\n",
    "        self.movieSoup = BeautifulSoup(self.movieHomepageHTML, parser=parser)\n",
    "        self.movieContentJSON = self._extractContent()\n",
    "        \n",
    "        \"\"\"For metadata\"\"\"\n",
    "        self.movieTitle = self.movieContentJSON['name']\n",
    "        self.movieTitle = ''.join(ch for ch in self.movieTitle if (ch.isalnum() or ch == ' ')) \n",
    "        print(self.movieTitle)\n",
    "        self.movieIMDbRating, self.totalReviews = self.movieContentJSON['aggregateRating']['ratingValue'], self.movieContentJSON['aggregateRating']['ratingCount']\n",
    "        self.totalUserReviews, self.totalCriticReviews, self.metaScore = self._extractMovieScores()\n",
    "        self.movieDatePublished = self.movieContentJSON['datePublished']\n",
    "        self.movieGenres = self.movieContentJSON['genre']\n",
    "        self.movieDirectors = self._extractDirectors()\n",
    "        self.movieWriters = self._extractWriters()\n",
    "        self.movieStars = self._extractStars()\n",
    "        self.movieDescription = self._extractDescription()\n",
    "        self.movieDuration = self._extractDuration()\n",
    "        \n",
    "        \"\"\"For reviews\"\"\"\n",
    "        self.reviewsURL = self.movieURL + 'reviews'\n",
    "        self.reviewsSoup = self._fullPageSoup()\n",
    "        self.reviewsExtracted = self._extractReviewsInfo()\n",
    "        \n",
    "        \"\"\"Compilation\"\"\"\n",
    "        self.metaData = self._prepareMetadata()\n",
    "        self.reviewsExtracted = self._prepareReviewsDataframe()\n",
    "        \n",
    "        \"\"\"Export\"\"\"\n",
    "        self._exportData()\n",
    "        \n",
    "\n",
    "    \"\"\"For metadeta\"\"\"\n",
    "    def _simpleConnector(self, url):\n",
    "        try:\n",
    "            html = requests.get(url)\n",
    "            html.raise_for_status()\n",
    "        except HTTPErrorRequests as e:\n",
    "            print('HTTP ERROR')\n",
    "            print(e)\n",
    "            return None\n",
    "        except ConnectionError as e:\n",
    "            print('CONNECTION ERROR')\n",
    "            print(e)\n",
    "            return None\n",
    "        except Timeout as e:\n",
    "            print('TIMEOUT')\n",
    "            print(e)\n",
    "            return None\n",
    "        except TooManyRedirects as e:\n",
    "            print('BAD URL: TOO MANY REDIRECTS')\n",
    "            print(e)\n",
    "            return None\n",
    "        except RequestException as e:\n",
    "            raise SystemExit(e)\n",
    "        else:\n",
    "            print('URL Connected')\n",
    "            return html.text\n",
    "    \n",
    "    def _extractMovieID(self):\n",
    "        movieID = self.movieURL.split('/')[-2]\n",
    "        print('Movie ID Extraction done')\n",
    "        return movieID\n",
    "    \n",
    "    def _extractContent(self):\n",
    "        content = loads(self.movieSoup.find('script', {'type': 'application/ld+json'}).text)\n",
    "        print('Content Extraction done')\n",
    "        return content\n",
    "    \n",
    "    def _extractMovieScores(self):\n",
    "        all_reviews = self.movieSoup.find('ul', {'data-testid': 'reviewContent-all-reviews'}).find_all('li')\n",
    "        stats = []\n",
    "        for review in all_reviews:\n",
    "            score = review.find('span', {'class': 'score'}).text\n",
    "            print(score)\n",
    "            stats.append(score)\n",
    "            \n",
    "        return stats\n",
    "    \n",
    "    def _extractDirectors(self):\n",
    "        directors = self.movieContentJSON['director']\n",
    "        \n",
    "        directors_list = []\n",
    "        for director in directors:\n",
    "            if 'name' not in director:\n",
    "                continue\n",
    "            directors_list.append(director['name'])\n",
    "            \n",
    "        print('Directors Extraction done!')\n",
    "        print(directors_list)\n",
    "        return directors_list\n",
    "    \n",
    "    def _extractWriters(self):\n",
    "        writers = self.movieContentJSON['creator']\n",
    "        names = []\n",
    "        for name in writers:\n",
    "            if 'name' not in name:\n",
    "                continue\n",
    "            names.append(name['name'])\n",
    "        print(\"Writers extracted!\")\n",
    "        print(names)\n",
    "        return names\n",
    "    \n",
    "    def _extractStars(self):\n",
    "        stars = self.movieContentJSON['actor']\n",
    "        actors = []\n",
    "        \n",
    "        for star in stars:\n",
    "            if 'name' not in star:\n",
    "                continue\n",
    "            actors.append(star['name'])\n",
    "        \n",
    "        print('Stars Extracted')\n",
    "        print(actors)\n",
    "        return actors\n",
    "    \n",
    "    def _extractDescription(self):\n",
    "        description = unescape(self.movieContentJSON['description'])\n",
    "        print(description)\n",
    "        return description\n",
    "    \n",
    "    def _extractDuration(self):\n",
    "        duration_split = self.movieContentJSON['duration'][2:].split('H')\n",
    "        hours = int(duration_split[0])\n",
    "        minutes = int(duration_split[1][:-1])\n",
    "        \n",
    "        duration = hours*60 + minutes\n",
    "        \n",
    "        return duration\n",
    "    \n",
    "    \"\"\"For reviews\"\"\"\n",
    "    def _fullPageSoup(self):\n",
    "        driver = webdriver.Chrome('C:\\\\chromedriver.exe')\n",
    "        driver.get(self.reviewsURL)\n",
    "\n",
    "        while True:\n",
    "            link = driver.find_element(By.CLASS_NAME, \"ipl-load-more__button\")\n",
    "            link.click()\n",
    "            time.sleep(9)\n",
    "            if link.value_of_css_property('display') == 'none':\n",
    "                break\n",
    "\n",
    "        souper = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        driver.quit()\n",
    "        print('Full Soup Extracted from User Reviews Page!')\n",
    "        \n",
    "        return souper\n",
    "    \n",
    "    def _extractReviewsInfo(self):\n",
    "        def extractHelpfulness(review):\n",
    "            review = review.split()\n",
    "            useful = review[0]\n",
    "            total = review[3]\n",
    "\n",
    "            useful = useful.replace(',', '')\n",
    "            total = total.replace(',', '')\n",
    "\n",
    "            return int(useful), int(total)\n",
    "        \n",
    "        list_collapsable = self.reviewsSoup.find_all('div', {'class': 'lister-item mode-detail imdb-user-review with-spoiler'})\n",
    "        list_with_spoiler = self.reviewsSoup.find_all('div', {'class': 'lister-item mode-detail imdb-user-review collapsable'})\n",
    "        \n",
    "        reviews = list_collapsable + list_with_spoiler\n",
    "        \n",
    "        complete_data = []\n",
    "        \n",
    "        for review in reviews:\n",
    "            try:\n",
    "                temp = []\n",
    "                reviewRating = review.find('span', {'class': None})\n",
    "                if reviewRating == None:\n",
    "                    continue\n",
    "                reviewDate = review.find('span', {'class':'review-date'}).get_text()\n",
    "                reviewDesc = review.find('div', {'class': 'text'}).get_text()\n",
    "                reviewUser = review.find('span', {'class': 'display-name-link'}).get_text()\n",
    "                reviewTitle = review.find('a', {'class': 'title'}).get_text()\n",
    "                reviewFooter = review.find('div', {'class': 'actions text-muted'}).get_text()\n",
    "                reviewUseful, reviewTotal = extractHelpfulness(reviewFooter)\n",
    "                \n",
    "                temp.append(reviewDate)\n",
    "                temp.append(reviewUser)\n",
    "                temp.append(reviewUseful)\n",
    "                temp.append(reviewTotal)\n",
    "                temp.append(reviewRating.get_text())\n",
    "                temp.append(reviewTitle.strip())\n",
    "                temp.append(reviewDesc.strip())\n",
    "                \n",
    "                complete_data.append(temp)\n",
    "            except Exception as E:\n",
    "                print(E)\n",
    "                continue\n",
    "        \n",
    "        print('User Reviews are extracted!')\n",
    "        return complete_data\n",
    "    \n",
    "    \"\"\"Compilation\"\"\"\n",
    "    \n",
    "    def _prepareMetadata(self):\n",
    "        dictionary = {\n",
    "            'title': self.movieTitle,\n",
    "            'movieIMDbRating': self.movieIMDbRating,\n",
    "            'totalRatingCount': self.totalReviews,\n",
    "            'totalUserReviews': self.totalUserReviews,\n",
    "            'totalCriticReviews': self.totalCriticReviews,\n",
    "            'metaScore': self.metaScore,\n",
    "            'movieGenres': self.movieGenres,\n",
    "            'directors': self.movieDirectors,\n",
    "            'datePublished': self.movieDatePublished,\n",
    "            'creators': self.movieWriters,\n",
    "            'mainStars': self.movieStars,\n",
    "            'description': self.movieDescription,\n",
    "            'duration': self.movieDuration\n",
    "        }\n",
    "        \n",
    "        return dictionary\n",
    "    \n",
    "    def _prepareReviewsDataframe(self):\n",
    "        df = pd.DataFrame(self.reviewsExtracted, columns=['Date of Review', 'User', 'Usefulness Vote', 'Total Votes', \"User's Rating out of 10\", 'Review Title', \"Review\"])\n",
    "        return df\n",
    "    \n",
    "    \"\"\"Exporting data\"\"\"\n",
    "    def _exportData(self):\n",
    "        if not os.path.isdir(self.movieTitle):\n",
    "            os.makedirs(self.movieTitle)\n",
    "        \n",
    "        self.reviewsExtracted.to_csv(self.movieTitle + '/movieReviews.csv', index=False)\n",
    "        \n",
    "        with open(self.movieTitle + '/metadata.json', 'w') as fp:\n",
    "            dump(self.metaData, fp)\n",
    "    \n",
    "    \"\"\"Getters\"\"\"\n",
    "    def getMovieSoup(self):\n",
    "        return self.movieSoup\n",
    "    def getReviewsSoup(self):\n",
    "        return self.reviewsSoup\n",
    "    def getReviews(self):\n",
    "        return self.reviewsExtracted\n",
    "    def getMetadata(self):\n",
    "        return self.metaData"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spiritual-board",
   "metadata": {},
   "source": [
    "# Test URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "automated-reserve",
   "metadata": {},
   "outputs": [],
   "source": [
    "movieURLs = [\n",
    "    \"https://www.imdb.com/title/tt10872600/\", #Spiderman: No Way Home\n",
    "    \"https://www.imdb.com/title/tt4154796/\", #Avengers: Endgame\n",
    "    \"https://www.imdb.com/title/tt7286456/\", #Joker\n",
    "    \"https://www.imdb.com/title/tt0109830/\", #Forrest Gump\n",
    "    \"https://www.imdb.com/title/tt0468569/\" #The Dark Knight\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meaning-daniel",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enormous-small",
   "metadata": {},
   "source": [
    "Steps to follow first: <br>\n",
    "<ol>\n",
    "    <li>Download the respective libraries mentioned in \"Imported Libraries\" section. </li>\n",
    "    <li>Download chromedriver.exe and specify its path in the function <code>_fullPageSoup(self).</code></li>\n",
    "    <li>Paste the link of Movies' Main IMDb page in the movieURLs list.</li>\n",
    "    <li>Run the code below to start the scrapping process. The data will be stored automatically by making a folder of movie's name.</li>\n",
    "</ol> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saved-tattoo",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run the below code to start scrapping\n",
    "for url in movieURLs:\n",
    "    movie = MovieIMDb(url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
